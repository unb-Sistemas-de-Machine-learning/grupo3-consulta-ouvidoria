# ğŸ”„ Switch to Lighter Model (Fix Memory Error)

## Problem
```
model requires more system memory (50.0 GiB) than is available (8.6 GiB)
```

## Solution: Use `gemma2:2b` (1.6GB only!)

---

## âœ… Quick Fix (Already Applied)

Changed model from `phi3` (3.8GB) to **`gemma2:2b`** (1.6GB)

### Files Updated:
1. âœ… `config.py` - Changed `OLLAMA_MODEL` to `"gemma2:2b"`
2. âœ… `docker-compose.yml` - Updated download command

---

## ğŸš€ How to Apply Changes

### Option 1: Clean Start (Recommended)

```bash
# Stop and remove everything (including old model)
docker-compose down -v

# Start fresh with new model
docker-compose up
```

**This will:**
- âœ… Remove the old phi3 model (frees space)
- âœ… Download gemma2:2b (only 1.6GB)
- âœ… Start the app with the lighter model

### Option 2: Keep Data, Just Change Model

```bash
# Stop containers
docker-compose down

# Remove only the Ollama volume (keeps your Qdrant data)
docker volume rm grupo3-consulta-ouvidoria_ollama_data

# Start with new model
docker-compose up
```

---

## ğŸ“Š Available Lightweight Models

| Model | Size | RAM Needed | Quality | Best For |
|-------|------|------------|---------|----------|
| **gemma2:2b** â­ | 1.6GB | 4GB+ | â­â­â­ | 8GB RAM systems |
| **qwen2.5:1.5b** | 1.5GB | 4GB+ | â­â­â­ | Ultra light |
| **phi3:mini** | 2.3GB | 6GB+ | â­â­â­â­ | Better quality |
| **tinyllama** | 637MB | 2GB+ | â­â­ | Very basic tasks |

---

## ğŸ”§ Want to Try a Different Model?

### Change to qwen2.5:1.5b (Lightest):

**config.py:**
```python
OLLAMA_MODEL: str = "qwen2.5:1.5b"
```

**docker-compose.yml:**
```yaml
curl -X POST http://ollama:11434/api/pull -d '{"name":"qwen2.5:1.5b"}' &&
```

### Change to tinyllama (Ultra Light):

**config.py:**
```python
OLLAMA_MODEL: str = "tinyllama"
```

**docker-compose.yml:**
```yaml
curl -X POST http://ollama:11434/api/pull -d '{"name":"tinyllama"}' &&
```

Then restart:
```bash
docker-compose down -v
docker-compose up
```

---

## ğŸ› Troubleshooting

### Still getting memory errors?

**Check Docker RAM allocation:**
```bash
# Docker Desktop > Settings > Resources
# Increase Memory to at least 6GB
```

### Remove all models manually:

```bash
# Enter Ollama container
docker-compose exec ollama bash

# List models
ollama list

# Remove old models
ollama rm phi3
ollama rm llama3
# etc...

# Exit
exit
```

### Check available space:

```bash
# Check Docker disk usage
docker system df

# Clean up unused data
docker system prune -a --volumes
```

---

## âš¡ Performance Expectations

### gemma2:2b (Recommended for 8GB RAM):

| Metric | Performance |
|--------|-------------|
| Download Size | 1.6GB |
| RAM Usage | ~2-3GB |
| Response Time | 3-8 seconds |
| Quality | Good for Portuguese |
| JSON Generation | âœ… Reliable |

### System Requirements:
- **Minimum RAM:** 4GB free
- **Recommended RAM:** 6GB free
- **Disk Space:** 3GB free

---

## ğŸ“ What Changed?

### Before:
```python
OLLAMA_MODEL: str = "phi3"  # Was trying to download large variant (50GB?)
```

### After:
```python
OLLAMA_MODEL: str = "gemma2:2b"  # Explicit small version (1.6GB)
```

---

## ğŸ¯ Test After Switching

```bash
# 1. Start with new model
docker-compose up

# 2. Wait for download (1-2 minutes for 1.6GB)

# 3. Open browser
open http://localhost:8501

# 4. Test chat
Click "ğŸ’¬ Ajuda" â†’ Type: "OlÃ¡!"

# Should respond within 5-8 seconds âœ…
```

---

## ğŸ’¾ Memory Usage Comparison

```
With phi3 (wrong variant):
â”œâ”€â”€ System: 8.6GB available
â””â”€â”€ Model needs: 50GB âŒ FAIL!

With gemma2:2b:
â”œâ”€â”€ System: 8.6GB available
â”œâ”€â”€ Model: ~1.6GB
â”œâ”€â”€ Embeddings: ~1GB
â”œâ”€â”€ Streamlit: ~500MB
â”œâ”€â”€ Qdrant: ~200MB
â””â”€â”€ Total: ~3.3GB âœ… SUCCESS!
```

---

## ğŸš¨ Important Notes

1. **Download time:** ~1-2 minutes for 1.6GB (vs 30+ min for 50GB)
2. **Quality trade-off:** Smaller model = slightly less sophisticated responses
3. **Portuguese support:** gemma2:2b has good multilingual support
4. **JSON reliability:** Tested and works well for structured outputs

---

## ğŸ‰ After Switching

You should see:
```
âœ… "Baixando modelo gemma2:2b (1.6GB - leve para 8GB RAM)..."
âœ… "Modelo pronto! Iniciando aplicaÃ§Ã£o..."
âœ… "Ollama conectado: gemma2:2b @ http://ollama:11434"
```

App will be responsive and fast! ğŸš€

---

## Need Even Lighter?

If gemma2:2b still has issues, try:

```python
# Ultra minimal (637MB)
OLLAMA_MODEL: str = "tinyllama"
```

Note: Quality will be lower, but it will run on 2GB RAM.

