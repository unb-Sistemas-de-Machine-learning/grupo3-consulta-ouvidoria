{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syp8E0ShZ4JP"
      },
      "source": [
        "# Pipeline RAG - OuvidorIA\n",
        "\n",
        "Este notebook implementa o núcleo de inteligência do **OuvidorIA**. Aqui configuramos o fluxo de **RAG (Retrieval-Augmented Generation)** que permite ao assistente responder perguntas com base nos manuais oficiais.\n",
        "\n",
        "**Etapas:**\n",
        "1.  **Ingestão:** Carregamento de textos (simulando manuais do Fala.BR).\n",
        "2.  **Indexação:** Criação de Embeddings e armazenamento no Qdrant.\n",
        "3.  **Retrieval (Recuperação):** Busca semântica dos trechos relevantes.\n",
        "4.  **Generation (Geração):** Criação de respostas com LLM (Gemini).\n",
        "5.  **Testes:** Validação do fluxo com perguntas de usuários.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yBNQ4sIZ4JS"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index llama-index-llms-google-genai llama-index-embeddings-huggingface llama-index-vector-stores-qdrant qdrant-client -q\n",
        "!pip install -U \"llama-index\" \"llama-index-llms-google-genai\" \"google-genai\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "BnuuA8yIZ4JT",
        "outputId": "032fd83a-d385-487c-d568-861e3db71ad7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'qdrant_client'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3588081895.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mqdrant_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStorageContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_stores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqdrant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQdrantVectorStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle_genai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleGenerativeAI\u001b[0m \u001b[0;31m# Import atualizado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'qdrant_client'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import qdrant_client\n",
        "from llama_index.core import VectorStoreIndex, Document, Settings, StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.llms.google_genai import GoogleGenerativeAI # Import atualizado\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "\n",
        "# --- CONFIGURAÇÃO ---\n",
        "GOOGLE_API_KEY = \"\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "import importlib, pkgutil\n",
        "modname = \"llama_index.llms.google_genai\"\n",
        "m = importlib.import_module(modname)\n",
        "print(\"m.__file__:\", getattr(m, \"__file__\", \"unknown\"))\n",
        "print(\"exports:\", [n for n in dir(m) if not n.startswith(\"_\")])\n",
        "\n",
        "# 1. Configuração do Modelo de Embeddings (Gratuito/Open Source para economizar custos)\n",
        "# Usaremos um modelo multilíngue leve da HuggingFace\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# 2. Configuração da LLM (Gemini 1.5 Flash - Rápido e Eficiente)\n",
        "# Usando a classe GoogleGenerativeAI compatível com os novos modelos\n",
        "Settings.llm = GoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.2)\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        ")\n",
        "\n",
        "llm = GoogleGenAI(\n",
        "    model=\"models/gemini-1.5-flash\",   # ou \"gemini-1.5-flash\" conforme o fornecedor\n",
        "    api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "# Exemplo mínimo de uso\n",
        "resp = llm.complete(\"Escreva um haicai sobre programação\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR3yqEWmZ4JU"
      },
      "source": [
        "## 1. Criação da Base de Conhecimento (Simulação)\n",
        "Como não estamos carregando PDFs externos neste ambiente de teste, criaremos documentos manuais com informações reais extraídas do **Manual do Fala.BR**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zXemHb9Z4JU"
      },
      "outputs": [],
      "source": [
        "docs_text = [\n",
        "    # Documento 1: Definições Básicas\n",
        "    Document(\n",
        "        text=\"\"\"\n",
        "        O que é uma Ouvidoria?\n",
        "        A ouvidoria é um canal de interlocução entre o cidadão e a Administração Pública.\n",
        "        Sua finalidade é receber manifestações dos cidadãos, analisar, orientar e encaminhar às áreas responsáveis pelo tratamento ou apuração, respondendo ao manifestante.\n",
        "        A ouvidoria atua no pós-atendimento, ou seja, quando o cidadão já procurou o serviço e não obteve resposta ou a resposta não foi satisfatória.\n",
        "        \"\"\",\n",
        "        metadata={\"fonte\": \"Manual de Ouvidoria Pública\", \"capitulo\": \"Introdução\"}\n",
        "    ),\n",
        "    # Documento 2: Tipos de Manifestação\n",
        "    Document(\n",
        "        text=\"\"\"\n",
        "        Tipos de Manifestação no Fala.BR:\n",
        "        1. Denúncia: Comunicação de prática de ato ilícito cuja solução dependa da atuação de órgão de controle. Ex: uso indevido de carro oficial.\n",
        "        2. Reclamação: Demonstração de insatisfação relativa a serviço público. Ex: demora no atendimento, falta de remédio.\n",
        "        3. Solicitação: Requerimento de adoção de providência por parte da Administração. Ex: conserto de buraco na rua.\n",
        "        4. Sugestão: Proposta de aprimoramento de políticas e serviços públicos.\n",
        "        5. Elogio: Demonstração de satisfação ou reconhecimento.\n",
        "        \"\"\",\n",
        "        metadata={\"fonte\": \"Manual Fala.BR\", \"capitulo\": \"Tipos de Manifestação\"}\n",
        "    ),\n",
        "    # Documento 3: Prazos\n",
        "    Document(\n",
        "        text=\"\"\"\n",
        "        Prazos para Resposta:\n",
        "        O prazo para resposta às manifestações é de 30 (trinta) dias, prorrogável por mais 30 (trinta) dias, mediante justificativa expressa.\n",
        "        Para pedidos de Acesso à Informação (LAI), o prazo é de 20 (vinte) dias, prorrogável por mais 10 (dez) dias.\n",
        "        \"\"\",\n",
        "        metadata={\"fonte\": \"Lei 13.460/2017\", \"topico\": \"Prazos\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Documentos preparados: {len(docs_text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sio8828Z4JV"
      },
      "source": [
        "## 2. Indexação Vetorial (Qdrant)\n",
        "Aqui inicializamos o cliente Qdrant (em memória para este teste) e criamos o índice vetorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33CawnV1Z4JV"
      },
      "outputs": [],
      "source": [
        "# Inicializa cliente Qdrant em memória (RAM)\n",
        "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
        "\n",
        "# Cria o Vector Store\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"ouvidoria_knowledge\")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Indexa os documentos (Chunking -> Embedding -> Upsert no Qdrant)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    docs_text,\n",
        "    storage_context=storage_context,\n",
        ")\n",
        "\n",
        "print(\"Indexação concluída com sucesso no Qdrant!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uHzXoNaZ4JW"
      },
      "source": [
        "## 3. Implementação da Lógica de Retrieval & Generation\n",
        "Configuramos o *Query Engine* com um prompt personalizado para garantir que o chatbot aja como um especialista em ouvidoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z3gNQyOZ4JW"
      },
      "outputs": [],
      "source": [
        "# Definindo o Template do Prompt (System Prompt)\n",
        "qa_prompt_str = (\n",
        "    \"Você é o OuvidorIA, um assistente especializado em ajudar cidadãos com a Ouvidoria Pública.\\n\"\n",
        "    \"Use as informações de contexto abaixo para responder à dúvida do cidadão de forma clara, educada e direta.\\n\"\n",
        "    \"Se a resposta não estiver no contexto, diga que não sabe e oriente a buscar o site oficial da CGU.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"CONTEXTO:\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"PERGUNTA DO CIDADÃO: {query_str}\\n\"\n",
        "    \"RESPOSTA DO OUVIDORIA:\"\n",
        ")\n",
        "\n",
        "# Configurando o mecanismo de busca\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=2, # Busca os 2 trechos mais relevantes\n",
        "    text_qa_template=PromptTemplate(qa_prompt_str)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE4WEmfGZ4JW"
      },
      "source": [
        "## 4. Testes do Fluxo (Scripts de Validação)\n",
        "Vamos simular perguntas reais de usuários para validar se o RAG está recuperando o contexto correto e gerando boas respostas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxDKYLtaZ4JX"
      },
      "outputs": [],
      "source": [
        "def testar_rag(pergunta):\n",
        "    print(f\"\\nPERGUNTA: {pergunta}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Executa a pipeline\n",
        "    response = query_engine.query(pergunta)\n",
        "\n",
        "    # Exibe a resposta gerada\n",
        "    print(f\"RESPOSTA: {response}\\n\")\n",
        "\n",
        "    # Debug: Mostra quais documentos foram usados (Retrieval)\n",
        "    print(\"FONTES RECUPERADAS:\")\n",
        "    for node in response.source_nodes:\n",
        "        score = \"{:.2f}\".format(node.score)\n",
        "        fonte = node.metadata.get('fonte', 'N/A')\n",
        "        trecho = node.text[:100] + \"...\" # Primeiros 100 caracteres\n",
        "        print(f\"   - [Score: {score}] Fonte: {fonte} | Trecho: \\\"{trecho}\\\"\")\n",
        "\n",
        "# --- Cenários de Teste ---\n",
        "\n",
        "# Cenário 1: Dúvida sobre conceito\n",
        "testar_rag(\"Qual a diferença entre denúncia e reclamação?\")\n",
        "\n",
        "# Cenário 2: Dúvida sobre prazos\n",
        "testar_rag(\"Quanto tempo a ouvidoria tem para me responder?\")\n",
        "\n",
        "# Cenário 3: Pergunta fora do contexto (Teste de Alucinação)\n",
        "testar_rag(\"Qual a receita de bolo de cenoura?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}